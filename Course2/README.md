# Improving Deep Neural Networks

## Bias & Virance
* L2 Regularization
* Drop out
* Early Stopping

## Vanishing/Exploding Gradients
* He initialization for RELU
* Xavier initialization for tanh
* Other initialization

## Optimization
* Mini-batch
* Stochastic
* Momentum
* Bias Correction
* RMS Prop
* Adam
* Learning rate decay
* Hyperparameter
* Batch Norm
* Softmax
